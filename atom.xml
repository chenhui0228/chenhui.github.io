<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>陈辉的博客</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://202.104.112.179:8085/"/>
  <updated>2017-08-01T06:17:19.418Z</updated>
  <id>http://202.104.112.179:8085/</id>
  
  <author>
    <name>陈辉</name>
    <email>chenhui0228@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>linux-ops</title>
    <link href="http://202.104.112.179:8085/2017/08/01/linux-ops/"/>
    <id>http://202.104.112.179:8085/2017/08/01/linux-ops/</id>
    <published>2017-08-01T06:17:19.000Z</published>
    <updated>2017-08-01T06:17:19.418Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CEPH运维之安装部署（luminous）</title>
    <link href="http://202.104.112.179:8085/2017/08/01/ceph-ops/"/>
    <id>http://202.104.112.179:8085/2017/08/01/ceph-ops/</id>
    <published>2017-08-01T06:09:41.000Z</published>
    <updated>2017-08-03T13:22:48.726Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文档主要介绍ceph的搭建过程。</p>
<h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><p>服务器规划及配置，如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">hostname</th>
<th style="text-align:left">public ip</th>
<th style="text-align:left">cluster ip</th>
<th style="text-align:left">节点说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ch-osd-1</td>
<td style="text-align:left">172.16.30.73</td>
<td style="text-align:left">172.16.31.73</td>
<td style="text-align:left">osd节点</td>
</tr>
<tr>
<td style="text-align:left">ch-osd-2</td>
<td style="text-align:left">172.16.30.72</td>
<td style="text-align:left">172.16.31.72</td>
<td style="text-align:left">osd节点</td>
</tr>
<tr>
<td style="text-align:left">ch-osd-3</td>
<td style="text-align:left">172.16.30.75</td>
<td style="text-align:left">172.16.31.75</td>
<td style="text-align:left">osd节点</td>
</tr>
<tr>
<td style="text-align:left">ch-osd-4</td>
<td style="text-align:left">172.16.30.77</td>
<td style="text-align:left">172.16.31.77</td>
<td style="text-align:left">osd节点</td>
</tr>
<tr>
<td style="text-align:left">ch-mon-1</td>
<td style="text-align:left">172.16.30.78</td>
<td style="text-align:left">172.16.31.78</td>
<td style="text-align:left">mon+rgw+manger节点</td>
</tr>
<tr>
<td style="text-align:left">ch-mon-2</td>
<td style="text-align:left">172.16.30.79</td>
<td style="text-align:left">172.16.31.79</td>
<td style="text-align:left">mon+rgw节点</td>
</tr>
<tr>
<td style="text-align:left">ch-mon-3</td>
<td style="text-align:left">172.16.30.80</td>
<td style="text-align:left">172.16.31.80</td>
<td style="text-align:left">mon+rgw节点</td>
</tr>
</tbody>
</table>
<ul>
<li>操作系统：centos release 7.2</li>
<li>CPU：OSD节点为Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz * 48 ，MON节点为Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz * 32；</li>
<li>内存大小：OSD 为256GB； MON为64GB</li>
<li>数据硬盘配置（不含系统盘）：OSD 为1.2TB SAS * 3和480G SSD * 1，其中SSD不是必要的，我们这里主要存放journal，MON单独部署可以不需要数据盘</li>
<li>网络配置：public 网络和 cluster 均为万兆光纤</li>
<li>每台服务器第1，2块磁盘做RAID1；其余磁盘做RAID0</li>
<li>ch-mon-1节点作为管理节点，部署ceph-deploy</li>
<li>Ceph版本：目前最新版 v12.1.2</li>
<li>ceph-deploy版本：1.5.38</li>
<li>这里使用root用户安装，如果不是root用户，应该拥有root权限</li>
</ul>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="基础环境检查"><a href="#基础环境检查" class="headerlink" title="基础环境检查"></a>基础环境检查</h4><ol>
<li>网络连接正常（方法略）</li>
<li>ntp服务正常（方法略）</li>
<li>集群服务器时间，时区一致（方法略）</li>
<li>防火墙策略，开端口6789，6800：7300</li>
<li>SELINUX设置为Permissive或者禁掉</li>
<li>磁盘阵列检查</li>
</ol>
<p>首先需要在存储节点安装Megacli，下载RPM包，如MegaCli-8.07.14-1.noarch.rpm，安装命令如下：</p>
<pre><code>rpm -ivh MegaCli-8.07.14-1.noarch.rpm
</code></pre><p>使用磁盘阵列检查工具Megacli进行相关检查，查看是否满足需要的配置策略。</p>
<pre><code>/opt/MegaRAID/MegaCli64 -LDGetProp -Cache -LALL -aALL
</code></pre><p>一般情况可在安装操作系统前对各硬盘做好磁盘阵列，不同厂商的设备磁盘阵列配置略有不同，这里不做详述。如果没做磁盘阵列，这里需要做磁盘阵列，使用MegaCli来对磁盘做日常管理。</p>
<p>OSD数据盘做RAID0 ，并分别设置读、写、缓存策略，其中读策略为Read-Ahead；写策略为WriteBack，写入缓存直接返回；磁盘缓存Disk cache设置为disable；I/O策略为Direct请求不被系统Cache缓存。如磁盘编号为10 的物理磁盘组成一个RAID0</p>
<pre><code>/opt/MegaRAID/MegaCli64 -cfgldadd -r0 [32:10] WB RA RA -a0
</code></pre><p>或者</p>
<pre><code>/opt/MegaRAID/MegaCli64 -LDSetProp 10 WB -a0
/opt/MegaRAID/MegaCli64 -LDSetProp 10 RA -a0
/opt/MegaRAID/MegaCli64 -LDSetProp 10 RA -a0
</code></pre><p>SSD做日志盘做RAID0，读策略为Normal，不适用预读，写策略为Write Through直接写入磁盘；如果SSD有掉电保护，磁盘缓存Disk cache设置为Enable；I/O策略为Direct请求不被Cache缓存。</p>
<pre><code>/opt/MegaRAID/MegaCli64 -cfgldadd -r0 [32:13] WT NORA Direct -a0
</code></pre><p>Megacli更详细的使用，可以参考其他资料，如<a href="https://supportforums.cisco.com/document/62901/megacli-common-commands-and-procedures" target="_blank" rel="external">参考文档1</a>等</p>
<h4 id="基础环境配置"><a href="#基础环境配置" class="headerlink" title="基础环境配置"></a>基础环境配置</h4><h5 id="yum源配置"><a href="#yum源配置" class="headerlink" title="yum源配置"></a>yum源配置</h5><p>这里采用yum安装，我已经将CEPH二进制RPM包上传至YUM镜像，所以先配置CEPH可用的YUM源，包括CEPH本身和EPEL源，如下：</p>
<pre><code>[root@node-5 env-check-init]# cat /etc/yum.repos.d/CentOS.repo 
...
[CENTOS7-epel]
name=CENTOS7 epel resource
baseurl=http://yum17.int.sfdc.com.cn/epel7Server/$basearch
enabled=1
gpgcheck=0
gpgkey=http://yum17.int.sfdc.com.cn/epel7/$basearch/RPM-GPG-KEY-EPEL-7

[CENTOS7-ceph]
name=CENTOS7 ceph resource
baseurl=http://yum17.int.sfdc.com.cn/ceph/el7/$basearch
enabled=1
gpgcheck=0
gpgkey=http://yum17.int.sfdc.com.cn/ceph/el7/$basearch/RPM-GPG-KEY-EPEL-7
</code></pre><blockquote>
<p><strong>提示</strong>：如果你没有自己的YUM源可使用国内开源镜像，如<a href="https://mirrors.tuna.tsinghua.edu.cn/" target="_blank" rel="external">清华镜像</a>，<a href="http://mirrors.ustc.edu.cn/" target="_blank" rel="external">中科大镜像</a>，<a href="https://mirrors.aliyun.com/" target="_blank" rel="external">阿里镜像</a>等，也可以使用<a href="http://docs.ceph.com/docs/master/start/quick-start-preflight/#rhel-centos" target="_blank" rel="external">Ceph官方文档</a>给出的配置。</p>
</blockquote>
<h5 id="SSH互信"><a href="#SSH互信" class="headerlink" title="SSH互信"></a>SSH互信</h5><p>需要为管理节点和其他集群节点建立ssh互信，使管理节点可以免验证登录其他各节点</p>
<p>在管理节点生成ssh keys，命令如下：</p>
<pre><code>ssh-keygen
</code></pre><p>将管理节点的ssh key 拷贝到其它个节点：</p>
<pre><code>ssh-copy-id root@172.16.30.xxx
</code></pre><h3 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h3><h4 id="软件包安装"><a href="#软件包安装" class="headerlink" title="软件包安装"></a>软件包安装</h4><p>在管理节点安装ceph-deploy</p>
<pre><code>yum install ceph-deploy -y
</code></pre><p>安装Ceph，在各个节点执行命令</p>
<pre><code>yum install ceph -y
</code></pre><p>安装完后可以使用如下命令查看版本</p>
<pre><code>[root@ch-mon-1 ~]# ceph -v
ceph version 12.1.2 (b661348f156f148d764b998b65b90451f096cb27) luminous (rc)
</code></pre><p>同时，可以看到在/etc目录下新增了一个ceph目录。进入/etc/ceph目录</p>
<pre><code>cd /etc/ceph
</code></pre><h4 id="创建集群"><a href="#创建集群" class="headerlink" title="创建集群"></a>创建集群</h4><h5 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h5><p>准备3个Monitor节点</p>
<pre><code>ceph-deploy new ch-mon-1 ch-mon-2 ch-mon-3
</code></pre><p>执行完毕后再该目录下可以看到有如下文件</p>
<pre><code>[root@ch-mon-1 ceph]# pwd
/etc/ceph
[root@ch-mon-1 ceph]# ll
-rw-r--r-- 1 root root   805 Aug  3 13:44 ceph.conf
-rw-r--r-- 1 root root 33736 Aug  3 13:45 ceph-deploy-ceph.log
-rw------- 1 root root    73 Aug  3 13:43 ceph.mon.keyring
</code></pre><p>此时可以开始规划集群配置，如集群网络配置，我这里的配置如下：</p>
<p>默认配置</p>
<pre><code>[root@ch-mon-1 ceph]# cat ceph.conf 
[global]
fsid = 31fc3bef-d912-4d12-aa1e-130d3270d5db
mon_initial_members = ch-mon-1, ch-mon-2, ch-mon-3
mon_host = 172.16.30.78,172.16.30.79,172.16.30.80
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
</code></pre><p>修改后</p>
<pre><code>[root@ch-mon-1 ceph]# cat ceph.conf 
[global]
fsid = 31fc3bef-d912-4d12-aa1e-130d3270d5db
mon_initial_members = ch-mon-1, ch-mon-2, ch-mon-3
mon_host = 172.16.30.78,172.16.30.79,172.16.30.80
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

public_network = 172.16.30.0/24
cluster_network = 172.16.31.0/24
osd_pool_default_size = 3
osd_pool_default_min_size = 1
osd_pool_default_pg_num = 8
osd_pool_default_pgp_num = 8
osd_crush_chooseleaf_type = 1

[mon]
mon_clock_drift_allowed = 0.5

[osd]
osd_mkfs_type = xfs
osd_mkfs_options_xfs = -f
filestore_max_sync_interval = 5
filestore_min_sync_interval = 0.1
filestore_fd_cache_size = 655350
filestore_omap_header_cache_size = 655350
filestore_fd_cache_random = true
osd op threads = 8
osd disk threads = 4
filestore op threads = 8
max_open_files = 655350
</code></pre><h5 id="初始化Monitor"><a href="#初始化Monitor" class="headerlink" title="初始化Monitor"></a>初始化Monitor</h5><p>部署初始的monitors，并获得keys</p>
<pre><code>ceph-deploy mon create-initial
</code></pre><p>做完这一步，在当前目录下就会看到有如下的keyrings：</p>
<pre><code>[root@ch-mon-1 ceph]# ll
-rw------- 1 root root    71 Aug  3 13:45 ceph.bootstrap-mds.keyring
-rw------- 1 root root    71 Aug  3 13:45 ceph.bootstrap-mgr.keyring
-rw------- 1 root root    71 Aug  3 13:45 ceph.bootstrap-osd.keyring
-rw------- 1 root root    71 Aug  3 13:45 ceph.bootstrap-rgw.keyring
-rw------- 1 root root    63 Aug  3 13:45 ceph.client.admin.keyring
</code></pre><p>要在节点使用ceph命令行需要将ceph.client.admin.keyring放在需要的节点的/etc/ceph目录下。如这里希望在所有的节点使用命令行，可以通过如下命令将ceph.client.admin.keyring拷贝到各节点，当然也可以使用cp命令。</p>
<pre><code>ceph-deploy admin ch-mon-2 ch-mon-3 ch-osd-1 ch-osd-2 ch-osd-3 ch-osd-4
</code></pre><p>在L版本的Ceph中新增了manager daemon，如下命令部署一个Manager守护进程</p>
<pre><code>ceph-deploy mgr create ch-mon-1
</code></pre><h5 id="增加OSDs"><a href="#增加OSDs" class="headerlink" title="增加OSDs"></a>增加OSDs</h5><p>我们使用的版本后端存储默认使用bluestore</p>
<p>下面我们添加OSDs</p>
<pre><code>ceph-deploy osd create ch-osd-1:/dev/sdb ch-osd-1:/dev/sdc ch-osd-1:/dev/sdd ch-osd-2:/dev/sdb ch-osd-2:/dev/sdc ch-osd-2:/dev/sdd ch-osd-3:/dev/sdb ch-osd-3:/dev/sdc ch-osd-3:/dev/sdd ch-osd-4:/dev/sdb ch-osd-4:/dev/sdc ch-osd-4:/dev/sdd
</code></pre><blockquote>
<p><strong>提示</strong>：在早期的版本中，添加OSD分为prepare和activate两步，这里不详述</p>
</blockquote>
<p>等命令执行结束之后可以查看集群状态</p>
<pre><code>[root@ch-osd-1 ~]# ceph -s
  cluster:
    id:     31fc3bef-d912-4d12-aa1e-130d3270d5db
    health: HEALTH_WARN
            application not enabled on 1 pool(s)
            too few PGs per OSD (1 &lt; min 30)

  services:
    mon: 3 daemons, quorum ch-mon-1,ch-mon-2,ch-mon-3
    mgr: ch-mon-1(active)
    osd: 12 osds: 12 up, 12 in

  data:
    pools:   1 pools, 16 pgs
    objects: 1 objects, 499 bytes
    usage:   12742 MB used, 13386 GB / 13398 GB avail
    pgs:     16 active+clean
</code></pre><p>查看OSDs</p>
<pre><code>[root@ch-osd-1 ~]# ceph osd tree
ID CLASS WEIGHT   TYPE NAME         STATUS REWEIGHT PRI-AFF 
-1       13.08472 root default                              
-3        3.27118     host ch-osd-1                         
 0   hdd  1.09039         osd.0         up  1.00000 1.00000 
 1   hdd  1.09039         osd.1         up  1.00000 1.00000 
 2   hdd  1.09039         osd.2         up  1.00000 1.00000 
-5        3.27118     host ch-osd-2                         
 3   hdd  1.09039         osd.3         up  1.00000 1.00000 
 4   hdd  1.09039         osd.4         up  1.00000 1.00000 
 5   hdd  1.09039         osd.5         up  1.00000 1.00000 
-7        3.27118     host ch-osd-3                         
 6   hdd  1.09039         osd.6         up  1.00000 1.00000 
 7   hdd  1.09039         osd.7         up  1.00000 1.00000 
 8   hdd  1.09039         osd.8         up  1.00000 1.00000 
-9        3.27118     host ch-osd-4                         
 9   hdd  1.09039         osd.9         up  1.00000 1.00000 
10   hdd  1.09039         osd.10        up  1.00000 1.00000 
11   hdd  1.09039         osd.11        up  1.00000 1.00000 
</code></pre><p>至此，整个集群就搭建完毕。</p>
<p>通过对目前最新版本Ceph部署，可见比老版本比如生产上大量使用的Hammer版部署起来简单，当然这里没有太多的配置优化。</p>
<h3 id="常用运维"><a href="#常用运维" class="headerlink" title="常用运维"></a>常用运维</h3><h4 id="开启监控模块"><a href="#开启监控模块" class="headerlink" title="开启监控模块"></a>开启监控模块</h4><p>在配置文件/etc/ceph/ceph.conf中添加</p>
<pre><code>[mgr]
mgr modules = dashboard
</code></pre><p>设置dashboard的ip和端口</p>
<pre><code>ceph config-key put mgr/dashboard/server_addr 172.16.30.78
ceph config-key put mgr/dashboard/server_port 7000
</code></pre><p>重启mgr服务</p>
<pre><code>[root@ch-osd-1 ~]# systemctl restart ceph-mgr@ch-mon-1
</code></pre><h4 id="增加-删除-MONITORs"><a href="#增加-删除-MONITORs" class="headerlink" title="增加/删除 MONITORs"></a>增加/删除 MONITORs</h4><h5 id="增加MONITOR"><a href="#增加MONITOR" class="headerlink" title="增加MONITOR"></a>增加MONITOR</h5><h5 id="删除MONITOR"><a href="#删除MONITOR" class="headerlink" title="删除MONITOR"></a>删除MONITOR</h5><p>首先停需要删除的monitor守护进程，命令如下：</p>
<pre><code>service ceph -a stop mon.{mon-id}
#for example
service ceph -a stop mon.ch-mon-1
</code></pre><p>然后将monitor从集群删除</p>
<pre><code>ceph mon remove {mon-id}
#for example
ceph mon remove ch-mon-1
</code></pre><p>最后，删除ceph.conf中的该monitor配置</p>
<p>更详细的使用方法见<a href="http://docs.ceph.com/docs/master/rados/operations/add-or-rm-mons/#removing-monitors" target="_blank" rel="external">官方文档</a></p>
<h4 id="增加-删除-OSDs"><a href="#增加-删除-OSDs" class="headerlink" title="增加/删除 OSDs"></a>增加/删除 OSDs</h4><h5 id="增加OSD"><a href="#增加OSD" class="headerlink" title="增加OSD"></a>增加OSD</h5><h5 id="删除OSD"><a href="#删除OSD" class="headerlink" title="删除OSD"></a>删除OSD</h5><p>在删除OSD之前，OSD通常为up且in的状态，需要先将要删除的OSD踢出集群，让集群可以进行rebalancing，同时将数据拷贝到其它OSD上。 在<strong>管理节点</strong>执行OSD踢出集群命令如下：</p>
<pre><code>ceph osd out {osd-num}
#for example
ceph osd out 2
</code></pre><p>一旦OSD被踢出集群后，集群通过将要删除的OSD上的PGs迁出来进行rebalancing，我们可以通过如下命令来观察整个过程</p>
<pre><code>ceph -w
</code></pre><blockquote>
<p><strong>注意</strong>：在只有少数服务器的小集群中，比如我们测试的集群，在进行out操作可能使一些PGs始终在active+remapped状态，这时，应该将osd置回in,回到初始状态<br><code>ceph osd in {osd-num}</code><br>然后，不是out OSD，而是将OSD权重设置为0。<br><code>ceph osd crush reweight osd.{osd-num} 0</code><br>之后再观察数据迁移状态。out和权重置0的区别在于，第一种情况下，OSD权重不改变，后者桶的权重被更新。</p>
</blockquote>
<p>将OSD踢出集群后，OSD可能仍然处于up且out状态，在从配置中删除OSD之前，需要将该OSD服务停止，登录<strong>OSD节点</strong>，执行命令停止相关OSD服务</p>
<pre><code>ssh {osd-host}
sudo systemctl stop ceph-osd@{osd-num}
#for example
ssh ch-osd-1
systemctl stop ceph-osd@2
</code></pre><p>或者</p>
<pre><code>service ceph stop osd.2
</code></pre><p>或者</p>
<pre><code>kill -9 {pid}
</code></pre><p>最后需要将OSD从集群的CRUSH MAP中删除，同时删除其权限，并且从OSD MAP删除OSD，在<strong>管理节点</strong>执行命令：</p>
<pre><code>#remove osd from crush map
ceph osd crush remove {name}
#remove the osd authentication key
ceph auth del osd.{osd-num}
#remove the osd
ceph osd rm {osd-num}
#for example
ceph osd crush remove osd.2
ceph auth del osd.2
ceph osd rm 2
</code></pre><p>如果在ceph.conf文件中有该OSD的配置，还需要删除相关配置</p>
<p>更详细的使用方法见<a href="http://docs.ceph.com/docs/master/rados/operations/add-or-rm-osds/#removing-osds-manual" target="_blank" rel="external">官方文档</a></p>
<p><strong>原创申明</strong>：本文为博主原创，转载请注明出处！    </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文档主要介绍ceph的搭建过程。&lt;/p&gt;
&lt;h3 id=&quot;集群规划&quot;&gt;&lt;a href=&quot;#集群规划&quot; class=&quot;headerlink&quot; title=&quot;集群规划&quot;&gt;&lt;/a&gt;集群规划&lt;/h3&gt;&lt;p&gt;服务器规划及配置，如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr
    
    </summary>
    
      <category term="分布式存储" scheme="http://202.104.112.179:8085/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
      <category term="运维操作" scheme="http://202.104.112.179:8085/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/%E8%BF%90%E7%BB%B4%E6%93%8D%E4%BD%9C/"/>
    
      <category term="Ceph" scheme="http://202.104.112.179:8085/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/%E8%BF%90%E7%BB%B4%E6%93%8D%E4%BD%9C/Ceph/"/>
    
    
      <category term="ceph" scheme="http://202.104.112.179:8085/tags/ceph/"/>
    
      <category term="分布式存储" scheme="http://202.104.112.179:8085/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>一致性哈希算法</title>
    <link href="http://202.104.112.179:8085/2017/07/31/consistHash/"/>
    <id>http://202.104.112.179:8085/2017/07/31/consistHash/</id>
    <published>2017-07-31T03:22:28.393Z</published>
    <updated>2017-07-31T03:22:28.379Z</updated>
    
    <content type="html"><![CDATA[<p>既然有一致性哈希，就肯定还有不一致哈希，为啥平时没人说不一致哈希呢？因为常见的哈希都是不一致的，所以就不修饰了，到了一致性哈希才特殊加个描述词修饰一下。</p>
<p>哈希一般都是将一个大数字取模然后分散到不同的桶里，假设我们只有两个桶，有 2、3、4、5 四个数字，那么模 2 分桶的结果就是：</p>
<img src="/2017/07/31/consistHash/unconsist-hash-1.png" alt="unconsist-hash-1.png" title="">
<p>这时我们嫌桶太少要给哈希表扩容加了一个新桶，这时候所有的数字就需要模 3 来确定分在哪个桶里，结果就变成了：</p>
<img src="/2017/07/31/consistHash/unconsist-hash-2.png" alt="unconsist-hash-2.png" title="">
<p>可以看到新加了一个桶后所有数字的分布都变了，这就意味着哈希表的每次扩展和收缩都会导致所有条目分布的重新计算，这个特性在某些场景下是不可接受的。比如分布式的存储系统，每个桶就相当于一个机器，文件分布在哪台机器由哈希算法来决定，这个系统想要加一台机器时就需要停下来等所有文件重新分布一次才能对外提供服务，而当一台机器掉线的时候尽管只掉了一部分数据，但所有数据访问路由都会出问题。这样整个服务就无法平滑的扩缩容，成为了有状态的服务。</p>
<p>要想实现无状态化，就要用到一致性哈希了，一致性哈希中假想我们有很多个桶，先定一个小目标比如 7 个，但一开始真实还是只有两个桶，编号是 3 和 6。哈希算法还是同样的取模，只不过现在分桶分到的很可能是不存在的桶，那么就往下找找到第一个真实存在的桶放进去。这样 2 和 3 都被分到了编号为 3 的桶， 4 和 5 被分到了编号为 6 的桶。</p>
<img src="/2017/07/31/consistHash/consist-hash-1.png" alt="consist-hash-1.png" title="">
<p>这时候再添加一个新的桶，编号是 4，取模方法不变还是模 7：</p>
<img src="/2017/07/31/consistHash/consist-hash-2.png" alt="consist-hash-2.png" title="">
<p>因为 3 号桶里都是取模小于等于 3 的，4 号桶只需要从 6 号桶里拿走属于它的数字就可以了，这种情况下只需要调整一个桶的数字就可分成了重新分布。可以想象下即使有 1 亿个桶，增加减少一个桶也只会影响一个桶的数据分布。</p>
<p>这样增加一个机器只需要和他后面的机器同步一下数据就可以开始工作了，下线一个机器需要先把他的数据同步到后面一台机器再下线。如果突然掉了一台机器也只会影响这台机器上的数据。实现中可以让每台机器同步一份自己前面机器的数据，这样即使掉线也不会影响这一部分的数据服务。</p>
<p>这里还有个小问题要是编号为 6 的机桶下线了，它没有后一个桶了，数据该咋办？为了解决这个问题，实现上通常把哈希空间做成环状，这样 3 就成了 6 的下一桶，数据给 3 就好了：</p>
<img src="/2017/07/31/consistHash/consist-hash-3.png" alt="consist-hash-3.png" title="">
<p>用一致性哈希还能实现部分的分布式系统无锁化，每个任务有自己的编号，由于哈希算法的确定性，分到哪个桶也是确定的就不存在争抢，也就不需要分布式锁了。</p>
<p>既然一致性哈希有这么多好的特性，那为啥主流的哈希都是非一致的呢？主要一个原因在于查找效率上，普通的哈希查询一次哈希计算就可以找到对应的桶了，算法时间复杂度是 O(1)，而一致性哈希需要将排好序的桶组成一个链表，然后一路找下去，k 个桶查询时间复杂度是 O(k)，所以通常情况下的哈希还是用不一致的实现。</p>
<p>当然 O(k) 的时间复杂度对于哈希来说还是不能忍的，想一下都是O(k) 这个量级了用哈希的意义在哪里？既然是在排好序的桶里查询，很自然的想法就是二分了，能把时间复杂度降到 O(logk)，然而桶的组合需要不断的增减，所以是个链表的实现，二分肯定就不行了，还好可以用跳转表进行一个快速的跳转也能实现 O(logk) 的时间复杂度。</p>
<img src="/2017/07/31/consistHash/finger-table.png" alt="finger-table.png" title="">
<p>在这个跳转表中，每个桶记录距离自己 1，2，4 距离的数字所存的桶，这样不管查询落在哪个节点上，对整个哈希环上任意的查询一次都可以至少跳过一半的查询空间，这样递归下去很快就可以定位到数据是存在哪个桶上。</p>
<p>当然这写都只是一致性哈希实现方式中的一种，还有很多实现上的变体。比如选择数字放在哪个桶，上面的介绍里是选择顺着数字下去出现的第一个桶，其实也可以选择距离这个数字最近的桶，这样实现和后面的跳转表规则也会有变化。同样跳转表也有多种不同的算法实现，感兴趣的可以去看一下 CAN，Chord，Tapestry，Pastry 这四种 DHT 的实现，有意思的是它们都是 2001 年发出来的 paper，所以 2001 年大概是 P2P 下载的元年吧。</p>
<p><strong>转载申明</strong>：本文转载博主<a href="http://weibo.com/oilbeater/profile?s=6cm7D0" target="_blank" rel="external">oilbeater</a>的文章<a href="http://oilbeater.com/%E5%8D%9A%E5%AE%A2/2016/12/18/consist-hash.html" target="_blank" rel="external">聊聊一致性哈希</a>，转载请注明出处！</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;既然有一致性哈希，就肯定还有不一致哈希，为啥平时没人说不一致哈希呢？因为常见的哈希都是不一致的，所以就不修饰了，到了一致性哈希才特殊加个描述词修饰一下。&lt;/p&gt;
&lt;p&gt;哈希一般都是将一个大数字取模然后分散到不同的桶里，假设我们只有两个桶，有 2、3、4、5 四个数字，那么模
    
    </summary>
    
      <category term="转载" scheme="http://202.104.112.179:8085/categories/%E8%BD%AC%E8%BD%BD/"/>
    
      <category term="技术原理" scheme="http://202.104.112.179:8085/categories/%E8%BD%AC%E8%BD%BD/%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="算法" scheme="http://202.104.112.179:8085/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="哈希" scheme="http://202.104.112.179:8085/tags/%E5%93%88%E5%B8%8C/"/>
    
      <category term="分布式" scheme="http://202.104.112.179:8085/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>这是一个测试文件</title>
    <link href="http://202.104.112.179:8085/2017/07/27/test/"/>
    <id>http://202.104.112.179:8085/2017/07/27/test/</id>
    <published>2017-07-27T07:55:34.936Z</published>
    <updated>2017-07-27T07:55:34.931Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个测试文件的内容</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一个测试文件的内容&lt;/p&gt;

    
    </summary>
    
      <category term="测试" scheme="http://202.104.112.179:8085/categories/%E6%B5%8B%E8%AF%95/"/>
    
    
      <category term="临时" scheme="http://202.104.112.179:8085/tags/%E4%B8%B4%E6%97%B6/"/>
    
      <category term="测试" scheme="http://202.104.112.179:8085/tags/%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
</feed>
